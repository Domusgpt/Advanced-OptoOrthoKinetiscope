# System Architecture: KineticLens v2.0

## 1. Architectural Pattern
KineticLens implements a **Unidirectional Data Flow** architecture specialized for high-frequency sensor fusion. The application logic is strictly separated into:
1.  **The Sensor Loop (60Hz):** `CaptureScreen.tsx` / `cpeMath.ts`
2.  **The State Machine:** `App.tsx`
3.  **The Render Pipeline:** `imageCompositor.ts`
4.  **The Inference Bridge:** `geminiService.ts`

## 2. Core Components

### 2.1 The Sensor Loop (`OrthogonalFilter`)
Located in `services/cpeMath.ts`, this class acts as the "Physics Engine".
*   **Input:** Raw `DeviceMotion` events.
*   **Process:**
    *   Maintains a rolling buffer of gravity vectors.
    *   Calculates the Principal Axis (Eigenvector of motion).
    *   Computes `VectorEfficiency`.
*   **Output:** A "Clean" `Vector3` acceleration used for both HUD visualization and AI analysis.
*   **Constraint:** Must execute in `<4ms` to maintain 60FPS render loop.

### 2.2 The Capture Buffer (`CaptureScreen.tsx`)
Because the Vision API is stateless, we must encapsulate all temporal context into a single payload.
*   **Buffer Structure:** `useRef<CapturePoint[]>`.
*   **Synchronization:** When the shutter fires (`grabFrame`), we assume the *current* sensor state applies to the *current* video frame.
    *   *Latency Compensation:* The `calibration.sensorReadoutLatency` variable (measured during setup) is used to offset the timestamp, aligning the photon capture time with the electron readout time.

### 2.3 The Holographic Compositor (`imageCompositor.ts`)
This component converts the temporal buffer into a spatial artifact.
*   **Input:** Array of JPEGs (Base64) + Telemetry Objects.
*   **Operation:** Uses HTML5 Canvas API to perform "Data Burning".
    *   It does NOT just draw text.
    *   It draws **Geometric Primitives** (Lines, Circles, Grids) that correspond to the sensor values.
*   **Why?** LLMs are better at "seeing" geometry (e.g., "The blue line crosses the red line") than they are at "reading" overlay text. We convert math into geometry.

## 3. Data Serialization & Export (`packageService.ts`)
To support "Offline Mode" and external audit, the system implements a zip-packaging protocol.
*   **Library:** `JSZip`.
*   **Artifacts:**
    1.  `composite_analysis_frame.jpg`: The fully rendered data-image.
    2.  `raw_session_data.json`: The complete `SessionData` object (dump of `types.ts`).
    3.  `INSTRUCTIONS_AND_PROMPT.txt`: The exact prompt string generated by `geminiService.ts`.
*   **Use Case:** This allows a user to capture data in the field (no internet) and perform the analysis later by uploading the files to a desktop LLM interface.

## 4. State Management (`types.ts`)
The `AppState` enum controls the high-level view routing.
```typescript
export enum AppState {
  IDLE,           // Landing
  INSTRUCTIONS,   // Permissions
  CALIBRATION,    // Sensor Zeroing
  CAPTURING,      // The 60Hz Loop
  ENCODING,       // Image Compositing
  PROCESSING,     // Gemini API Call
  ANALYZING,      // Decoding Response
  RESULTS         // Visualization
}
```
Transitions are linear and guarded. For example, you cannot enter `CAPTURING` without a valid `CalibrationData` object.
